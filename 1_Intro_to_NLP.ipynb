{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Intro to NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Sklearn"
      ],
      "metadata": {
        "id": "ImfdHCD_UjOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3ExSmx0cQCf7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# two example documents\n",
        "docs = [\"cat\", \"dog\", \"bat\", \"ate\"]"
      ],
      "metadata": {
        "id": "daUnGNXlQYx5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split documents to tokens\n",
        "tokens_docs = [doc.split(\" \") for doc in docs]\n",
        "print(tokens_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xTwg80JQfEG",
        "outputId": "507fbd8c-3642-4a4b-f290-8b50f9a680be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['cat'], ['dog'], ['bat'], ['ate']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert list of token-lists to one flat list of tokens\n",
        "# and then create a dictionary that maps word to id of word,\n",
        "all_tokens = itertools.chain.from_iterable(tokens_docs)\n",
        "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}"
      ],
      "metadata": {
        "id": "7te2Ar9bQw86"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mBalMleR09-",
        "outputId": "b7e293f6-2649-4d32-d5e9-c9ff650583e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ate': 0, 'bat': 1, 'cat': 2, 'dog': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert token lists to token-id lists\n",
        "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3rE8PyHR5cP",
        "outputId": "85a17d24-3514-4a7b-b201-d5a474531cd2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2], [3], [1], [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert list of token-id lists to one-hot representative\n",
        "\n",
        "vec = OneHotEncoder()\n",
        "x = vec.fit_transform(token_ids)\n",
        "print(x)\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca_13KzJSiQG",
        "outputId": "0392c1bc-8002-4c96-ff96-7c29f6472581"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t1.0\n",
            "  (1, 3)\t1.0\n",
            "  (2, 1)\t1.0\n",
            "  (3, 0)\t1.0\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sklearn CountVectorizer"
      ],
      "metadata": {
        "id": "qTMJ-A53UbVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = [\"i love nlp. nlp is so cool\"]\n",
        "\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "AYDN4GfAULTI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and built vocabulary\n",
        "\n",
        "vectorizer.fit(text)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJJI0Hm3U9hn",
        "outputId": "0e4b4b8a-da5f-430e-9b31-b9f97300229b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 2, 'nlp': 3, 'is': 1, 'so': 4, 'cool': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode document\n",
        "\n",
        "vector = vectorizer.transform(text)"
      ],
      "metadata": {
        "id": "mBn4SgDiVsEv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize encoded vector\n",
        "\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mp-OY6eWIoV",
        "outputId": "b1719f98-98ab-45ae-c411-202d1033e706"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n",
            "[[1 1 1 2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TfidVectorizer"
      ],
      "metadata": {
        "id": "XTJNBJq4WuKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text1 = ['i love nlp', 'nlp is so cool', \n",
        "'nlp is all about helping machines process language', \n",
        "'this tutorial is on baisc nlp technique']"
      ],
      "metadata": {
        "id": "s_RkJCoCWXhZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer()\n",
        "txt_fitted = tf.fit(text1)\n",
        "txt_transformed = txt_fitted.transform(text1)\n",
        "print (\"The text: \", text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LPPqzjGXjgF",
        "outputId": "32fa7a17-0669-4cec-eae9-edc422dd671c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text:  ['i love nlp', 'nlp is so cool', 'nlp is all about helping machines process language', 'this tutorial is on baisc nlp technique']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = tf.idf_\n",
        "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8qTHk0YC9d",
        "outputId": "2e4b1134-d157-4f9f-9ceb-fc2167b49368"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'about': 1.916290731874155, 'all': 1.916290731874155, 'baisc': 1.916290731874155, 'cool': 1.916290731874155, 'helping': 1.916290731874155, 'is': 1.2231435513142097, 'language': 1.916290731874155, 'love': 1.916290731874155, 'machines': 1.916290731874155, 'nlp': 1.0, 'on': 1.916290731874155, 'process': 1.916290731874155, 'so': 1.916290731874155, 'technique': 1.916290731874155, 'this': 1.916290731874155, 'tutorial': 1.916290731874155}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}